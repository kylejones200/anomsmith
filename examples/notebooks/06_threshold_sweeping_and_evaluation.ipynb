{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Threshold Sweeping and Evaluation\n",
    "\n",
    "This notebook demonstrates how to:\n",
    "1. Sweep multiple threshold values to find optimal thresholds\n",
    "2. Evaluate detection performance using precision, recall, and F1 scores\n",
    "3. Use ground truth labels to assess detector performance\n",
    "4. Visualize threshold selection trade-offs\n",
    "\n",
    "The `sweep_thresholds` function is particularly useful for:\n",
    "- Finding optimal threshold values\n",
    "- Understanding precision-recall trade-offs\n",
    "- Comparing different detectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from plotsmith import plot_timeseries\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from anomsmith import sweep_thresholds, detect_anomalies, ThresholdRule\n",
    "from anomsmith.primitives.scorers.robust_zscore import RobustZScoreScorer\n",
    "from anomsmith.primitives.scorers.statistical import ZScoreScorer, IQRScorer\n",
    "\n",
    "np.random.seed(42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Data with Ground Truth Labels\n",
    "\n",
    "For evaluation, we need data with known anomalies (ground truth labels).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_labeled_data(n: int = 200, contamination: float = 0.1, seed: int = 42):\n",
    "    \"\"\"Create data with known anomalies and ground truth labels.\"\"\"\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Base series\n",
    "    trend = np.linspace(0, 2, n)\n",
    "    noise = np.random.randn(n) * 0.5\n",
    "    y = trend + noise\n",
    "    \n",
    "    # Inject anomalies\n",
    "    n_anomalies = int(n * contamination)\n",
    "    anomaly_indices = np.random.choice(n, n_anomalies, replace=False)\n",
    "    y[anomaly_indices] += np.random.choice([-1, 1], n_anomalies) * np.random.uniform(4, 8, n_anomalies)\n",
    "    \n",
    "    # Create ground truth labels\n",
    "    labels = pd.Series(np.zeros(n), index=pd.date_range(\"2020-01-01\", periods=n, freq=\"D\"))\n",
    "    labels.iloc[anomaly_indices] = 1\n",
    "    \n",
    "    index = pd.date_range(\"2020-01-01\", periods=n, freq=\"D\")\n",
    "    y_series = pd.Series(y, index=index)\n",
    "    \n",
    "    return y_series, labels, anomaly_indices\n",
    "\n",
    "y, labels, true_anomaly_indices = create_labeled_data(n=200, contamination=0.1)\n",
    "print(f\"Created time series with {len(y)} points\")\n",
    "print(f\"True anomalies: {labels.sum()}\")\n",
    "print(f\"Anomaly rate: {labels.mean():.2%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize data with ground truth\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "ax.plot(y.index, y.values, 'b-', linewidth=1.5, alpha=0.7, label='Time Series')\n",
    "ax.scatter(y.index[true_anomaly_indices], y.values[true_anomaly_indices], \n",
    "          color='red', s=100, marker='x', linewidths=2, \n",
    "          label=f'True Anomalies ({len(true_anomaly_indices)})', zorder=5)\n",
    "ax.set_xlabel('Date', fontsize=12)\n",
    "ax.set_ylabel('Value', fontsize=12)\n",
    "ax.set_title('Time Series with Ground Truth Labels', fontsize=14, fontweight='bold')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sweeping Thresholds\n",
    "\n",
    "Let's sweep a range of threshold values and evaluate performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize scorer\n",
    "scorer = RobustZScoreScorer(epsilon=1e-8)\n",
    "scorer.fit(y.values)\n",
    "\n",
    "# Create range of threshold values (quantiles)\n",
    "threshold_values = np.linspace(0.5, 0.99, 50)\n",
    "\n",
    "# Sweep thresholds with ground truth labels\n",
    "sweep_results = sweep_thresholds(y, scorer, threshold_values, labels=labels)\n",
    "\n",
    "print(\"Threshold Sweep Results (first 10 rows):\")\n",
    "print(sweep_results.head(10))\n",
    "print(\"\\nThreshold Sweep Results (last 10 rows):\")\n",
    "print(sweep_results.tail(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find optimal threshold (max F1)\n",
    "optimal_idx = sweep_results['f1'].idxmax()\n",
    "optimal_threshold = sweep_results.loc[optimal_idx, 'threshold']\n",
    "optimal_f1 = sweep_results.loc[optimal_idx, 'f1']\n",
    "optimal_precision = sweep_results.loc[optimal_idx, 'precision']\n",
    "optimal_recall = sweep_results.loc[optimal_idx, 'recall']\n",
    "\n",
    "print(f\"Optimal Threshold: {optimal_threshold:.4f}\")\n",
    "print(f\"Optimal F1 Score: {optimal_f1:.4f}\")\n",
    "print(f\"Optimal Precision: {optimal_precision:.4f}\")\n",
    "print(f\"Optimal Recall: {optimal_recall:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize precision-recall curve\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# Precision-Recall curve\n",
    "ax1 = axes[0, 0]\n",
    "ax1.plot(sweep_results['recall'], sweep_results['precision'], 'b-', linewidth=2, alpha=0.7)\n",
    "ax1.scatter(optimal_recall, optimal_precision, color='red', s=200, marker='*', \n",
    "           zorder=5, label=f'Optimal (F1={optimal_f1:.3f})')\n",
    "ax1.set_xlabel('Recall', fontsize=12)\n",
    "ax1.set_ylabel('Precision', fontsize=12)\n",
    "ax1.set_title('Precision-Recall Curve', fontsize=14, fontweight='bold')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# F1 vs Threshold\n",
    "ax2 = axes[0, 1]\n",
    "ax2.plot(sweep_results['threshold'], sweep_results['f1'], 'g-', linewidth=2, alpha=0.7)\n",
    "ax2.axvline(optimal_threshold, color='r', linestyle='--', linewidth=2, \n",
    "           label=f'Optimal ({optimal_threshold:.3f})')\n",
    "ax2.set_xlabel('Threshold', fontsize=12)\n",
    "ax2.set_ylabel('F1 Score', fontsize=12)\n",
    "ax2.set_title('F1 Score vs Threshold', fontsize=14, fontweight='bold')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Precision vs Threshold\n",
    "ax3 = axes[1, 0]\n",
    "ax3.plot(sweep_results['threshold'], sweep_results['precision'], 'b-', linewidth=2, alpha=0.7, label='Precision')\n",
    "ax3.plot(sweep_results['threshold'], sweep_results['recall'], 'orange', linewidth=2, alpha=0.7, label='Recall')\n",
    "ax3.axvline(optimal_threshold, color='r', linestyle='--', linewidth=2)\n",
    "ax3.set_xlabel('Threshold', fontsize=12)\n",
    "ax3.set_ylabel('Score', fontsize=12)\n",
    "ax3.set_title('Precision and Recall vs Threshold', fontsize=14, fontweight='bold')\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# All metrics together\n",
    "ax4 = axes[1, 1]\n",
    "ax4.plot(sweep_results['threshold'], sweep_results['precision'], 'b-', linewidth=2, alpha=0.7, label='Precision')\n",
    "ax4.plot(sweep_results['threshold'], sweep_results['recall'], 'orange', linewidth=2, alpha=0.7, label='Recall')\n",
    "ax4.plot(sweep_results['threshold'], sweep_results['f1'], 'g-', linewidth=2, alpha=0.7, label='F1')\n",
    "ax4.axvline(optimal_threshold, color='r', linestyle='--', linewidth=2)\n",
    "ax4.set_xlabel('Threshold', fontsize=12)\n",
    "ax4.set_ylabel('Score', fontsize=12)\n",
    "ax4.set_title('All Metrics vs Threshold', fontsize=14, fontweight='bold')\n",
    "ax4.legend()\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing Different Scorers\n",
    "\n",
    "Let's compare how different scorers perform with threshold sweeping.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize multiple scorers\n",
    "scorers = {\n",
    "    'RobustZScore': RobustZScoreScorer(epsilon=1e-8),\n",
    "    'ZScore': ZScoreScorer(),\n",
    "    'IQR': IQRScorer()\n",
    "}\n",
    "\n",
    "# Fit all scorers\n",
    "for name, scorer in scorers.items():\n",
    "    scorer.fit(y.values)\n",
    "\n",
    "# Sweep thresholds for each scorer\n",
    "sweep_results_all = {}\n",
    "for name, scorer in scorers.items():\n",
    "    sweep_results_all[name] = sweep_thresholds(y, scorer, threshold_values, labels=labels)\n",
    "\n",
    "# Find optimal for each\n",
    "optimal_results = {}\n",
    "for name, results in sweep_results_all.items():\n",
    "    optimal_idx = results['f1'].idxmax()\n",
    "    optimal_results[name] = {\n",
    "        'threshold': results.loc[optimal_idx, 'threshold'],\n",
    "        'f1': results.loc[optimal_idx, 'f1'],\n",
    "        'precision': results.loc[optimal_idx, 'precision'],\n",
    "        'recall': results.loc[optimal_idx, 'recall']\n",
    "    }\n",
    "\n",
    "# Compare\n",
    "comparison_df = pd.DataFrame(optimal_results).T\n",
    "print(\"Optimal Performance Comparison:\")\n",
    "print(comparison_df.round(4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize comparison\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "colors = {'RobustZScore': 'blue', 'ZScore': 'green', 'IQR': 'orange'}\n",
    "\n",
    "for idx, metric in enumerate(['precision', 'recall', 'f1']):\n",
    "    ax = axes[idx]\n",
    "    for name, results in sweep_results_all.items():\n",
    "        ax.plot(results['threshold'], results[metric], \n",
    "               color=colors[name], linewidth=2, alpha=0.7, label=name)\n",
    "        # Mark optimal point\n",
    "        optimal_idx = results['f1'].idxmax()\n",
    "        ax.scatter(results.loc[optimal_idx, 'threshold'], \n",
    "                  results.loc[optimal_idx, metric],\n",
    "                  color=colors[name], s=100, marker='*', zorder=5)\n",
    "    \n",
    "    ax.set_xlabel('Threshold', fontsize=12)\n",
    "    ax.set_ylabel(metric.capitalize(), fontsize=12)\n",
    "    ax.set_title(f'{metric.capitalize()} vs Threshold', fontsize=14, fontweight='bold')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we've explored:\n",
    "1. **sweep_thresholds**: Evaluating multiple threshold values\n",
    "2. **Precision, Recall, F1**: Understanding evaluation metrics\n",
    "3. **Optimal threshold selection**: Finding the best threshold for your use case\n",
    "4. **Comparing scorers**: Evaluating different algorithms\n",
    "\n",
    "Key takeaways:\n",
    "- Threshold selection is crucial for good performance\n",
    "- Precision-recall trade-offs help understand detector behavior\n",
    "- Different scorers may have different optimal thresholds\n",
    "- F1 score balances precision and recall\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

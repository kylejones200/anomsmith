{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA-Based Anomaly Detection\n",
    "\n",
    "This notebook demonstrates the `PCADetector`, which uses Principal Component Analysis to model healthy operation boundaries and detect anomalies.\n",
    "\n",
    "PCA-based detection is particularly useful for:\n",
    "- Multivariate time series\n",
    "- High-dimensional data\n",
    "- When you want to capture the main patterns in normal data\n",
    "\n",
    "The detector supports three scoring methods:\n",
    "1. **Reconstruction error**: Measures how well data can be reconstructed from principal components\n",
    "2. **Mahalanobis distance**: Distance in the principal component space\n",
    "3. **Both**: Average of reconstruction error and Mahalanobis distance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from plotsmith import plot_timeseries\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from anomsmith import detect_anomalies, ThresholdRule\n",
    "from anomsmith.primitives.detectors.pca import PCADetector\n",
    "\n",
    "np.random.seed(42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Multivariate Test Data\n",
    "\n",
    "For PCA, we'll create data with multiple correlated features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_multivariate_data(n: int = 200, n_features: int = 5, contamination: float = 0.1, seed: int = 42):\n",
    "    \"\"\"Create multivariate time series with anomalies.\"\"\"\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Create correlated features\n",
    "    base = np.random.randn(n, n_features)\n",
    "    # Add correlation structure\n",
    "    correlation_matrix = np.random.rand(n_features, n_features)\n",
    "    correlation_matrix = correlation_matrix @ correlation_matrix.T\n",
    "    correlation_matrix = correlation_matrix / np.sqrt(np.diag(correlation_matrix))[:, None]\n",
    "    correlation_matrix = correlation_matrix / np.sqrt(np.diag(correlation_matrix))[None, :]\n",
    "    \n",
    "    data = base @ correlation_matrix.T\n",
    "    \n",
    "    # Add trend and seasonality to first feature\n",
    "    t = np.arange(n)\n",
    "    data[:, 0] += 0.01 * t + 2 * np.sin(2 * np.pi * t / 50)\n",
    "    \n",
    "    # Inject anomalies\n",
    "    n_anomalies = int(n * contamination)\n",
    "    anomaly_indices = np.random.choice(n, n_anomalies, replace=False)\n",
    "    \n",
    "    for idx in anomaly_indices:\n",
    "        # Anomalies affect multiple features\n",
    "        data[idx] += np.random.randn(n_features) * 3\n",
    "    \n",
    "    # Convert to DataFrame for easier handling\n",
    "    index = pd.date_range(\"2020-01-01\", periods=n, freq=\"D\")\n",
    "    df = pd.DataFrame(data, index=index, columns=[f'feature_{i}' for i in range(n_features)])\n",
    "    \n",
    "    # For PCA detector, we'll use the first feature as y and others as X\n",
    "    y = df['feature_0']\n",
    "    X = df.drop('feature_0', axis=1)\n",
    "    \n",
    "    return y, X, anomaly_indices\n",
    "\n",
    "y, X, true_anomaly_indices = create_multivariate_data(n=200, n_features=5, contamination=0.1)\n",
    "print(f\"Created multivariate data with {len(y)} points and {X.shape[1]} features\")\n",
    "print(f\"True anomalies: {len(true_anomaly_indices)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the multivariate data\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Main feature\n",
    "fig1, ax1 = plot_timeseries(\n",
    "    y,\n",
    "    title='Main Feature with Anomalies',\n",
    "    xlabel='Date',\n",
    "    ylabel='Value'\n",
    ")\n",
    "ax1.scatter(y.index[true_anomaly_indices], y.values[true_anomaly_indices], \n",
    "           color='red', s=100, marker='x', linewidths=2, \n",
    "           label=f'True Anomalies ({len(true_anomaly_indices)})', zorder=5)\n",
    "ax1.legend()\n",
    "plt.show()\n",
    "\n",
    "# Other features\n",
    "fig2, ax2 = plot_timeseries(\n",
    "    X.iloc[:, 0],\n",
    "    title='Additional Features',\n",
    "    xlabel='Date',\n",
    "    ylabel='Value'\n",
    ")\n",
    "for col in X.columns[1:]:\n",
    "    ax2.plot(X.index, X[col].values, alpha=0.6, label=col)\n",
    "ax2.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA Detection with Different Scoring Methods\n",
    "\n",
    "Let's compare the three scoring methods: reconstruction error, Mahalanobis distance, and both.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create PCA detectors with different scoring methods\n",
    "pca_reconstruction = PCADetector(\n",
    "    n_components=0.95,\n",
    "    score_method='reconstruction',\n",
    "    contamination=0.1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "pca_mahalanobis = PCADetector(\n",
    "    n_components=0.95,\n",
    "    score_method='mahalanobis',\n",
    "    contamination=0.1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "pca_both = PCADetector(\n",
    "    n_components=0.95,\n",
    "    score_method='both',\n",
    "    contamination=0.1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Fit detectors (using X as features, y as target)\n",
    "pca_reconstruction.fit(y.values, X.values)\n",
    "pca_mahalanobis.fit(y.values, X.values)\n",
    "pca_both.fit(y.values, X.values)\n",
    "\n",
    "# Define threshold rule\n",
    "threshold_rule = ThresholdRule(method=\"quantile\", value=0.9, quantile=0.9)\n",
    "\n",
    "# Detect anomalies\n",
    "result_recon = detect_anomalies(y, pca_reconstruction, threshold_rule)\n",
    "result_mahal = detect_anomalies(y, pca_mahalanobis, threshold_rule)\n",
    "result_both = detect_anomalies(y, pca_both, threshold_rule)\n",
    "\n",
    "# Compare results\n",
    "comparison = pd.DataFrame({\n",
    "    'Reconstruction': [\n",
    "        result_recon['flag'].sum(),\n",
    "        result_recon['flag'].mean(),\n",
    "        result_recon['score'].mean(),\n",
    "        result_recon['score'].std()\n",
    "    ],\n",
    "    'Mahalanobis': [\n",
    "        result_mahal['flag'].sum(),\n",
    "        result_mahal['flag'].mean(),\n",
    "        result_mahal['score'].mean(),\n",
    "        result_mahal['score'].std()\n",
    "    ],\n",
    "    'Both': [\n",
    "        result_both['flag'].sum(),\n",
    "        result_both['flag'].mean(),\n",
    "        result_both['score'].mean(),\n",
    "        result_both['score'].std()\n",
    "    ]\n",
    "}, index=['Anomalies Detected', 'Anomaly Rate', 'Mean Score', 'Std Score'])\n",
    "\n",
    "print(\"PCA Scoring Method Comparison:\")\n",
    "print(comparison.round(4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize detection results\n",
    "methods = [\n",
    "    ('Reconstruction Error', result_recon, 'blue'),\n",
    "    ('Mahalanobis Distance', result_mahal, 'green'),\n",
    "    ('Both (Average)', result_both, 'orange')\n",
    "]\n",
    "\n",
    "for name, result, color in methods:\n",
    "    anomaly_mask = result['flag'] == 1\n",
    "    fig, ax = plot_timeseries(\n",
    "        y,\n",
    "        title=f'PCA Detection: {name}',\n",
    "        xlabel='Date',\n",
    "        ylabel='Value'\n",
    "    )\n",
    "    # True anomalies\n",
    "    ax.scatter(y.index[true_anomaly_indices], y.values[true_anomaly_indices], \n",
    "              color='gray', s=80, marker='o', alpha=0.5, \n",
    "              label='True Anomalies', zorder=3)\n",
    "    # Detected anomalies\n",
    "    ax.scatter(y.index[anomaly_mask], y.values[anomaly_mask], \n",
    "              color='red', s=100, marker='x', linewidths=2, \n",
    "              label=f'Detected ({anomaly_mask.sum()})', zorder=5)\n",
    "    ax.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding PCA Components\n",
    "\n",
    "Let's examine how many components PCA is using and the explained variance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get PCA model from one of the detectors\n",
    "pca_model = pca_reconstruction.pca_\n",
    "print(f\"Number of components: {pca_model.n_components_}\")\n",
    "print(f\"Explained variance ratio: {pca_model.explained_variance_ratio_}\")\n",
    "print(f\"Total explained variance: {pca_model.explained_variance_ratio_.sum():.4f}\")\n",
    "\n",
    "# Visualize explained variance\n",
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ax.bar(range(1, len(pca_model.explained_variance_ratio_) + 1), \n",
    "       pca_model.explained_variance_ratio_, alpha=0.7)\n",
    "ax.set_xlabel('Principal Component', fontsize=12)\n",
    "ax.set_ylabel('Explained Variance Ratio', fontsize=12)\n",
    "ax.set_title('PCA Explained Variance by Component', fontsize=14, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we've explored:\n",
    "1. **PCADetector** with reconstruction error scoring\n",
    "2. **PCADetector** with Mahalanobis distance scoring\n",
    "3. **PCADetector** with combined scoring\n",
    "\n",
    "Key takeaways:\n",
    "- PCA is excellent for multivariate anomaly detection\n",
    "- Reconstruction error captures how well data fits the normal pattern\n",
    "- Mahalanobis distance measures distance in the principal component space\n",
    "- The choice of scoring method depends on your specific use case\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

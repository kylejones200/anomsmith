{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning-Based Anomaly Detectors\n",
    "\n",
    "This notebook demonstrates the machine learning-based anomaly detectors available in Anomsmith:\n",
    "\n",
    "1. **IsolationForestDetector**: Isolation Forest algorithm for detecting outliers\n",
    "2. **LOFDetector**: Local Outlier Factor for density-based anomaly detection\n",
    "3. **RobustCovarianceDetector**: Robust covariance (elliptic envelope) for multivariate anomaly detection\n",
    "\n",
    "These detectors are particularly useful for complex, high-dimensional data where statistical methods may not be sufficient.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from plotsmith import plot_timeseries\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from anomsmith import detect_anomalies, ThresholdRule\n",
    "from anomsmith.primitives.detectors.ml import (\n",
    "    IsolationForestDetector,\n",
    "    LOFDetector,\n",
    "    RobustCovarianceDetector\n",
    ")\n",
    "\n",
    "np.random.seed(42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Complex Test Data\n",
    "\n",
    "We'll create data with complex patterns that benefit from ML-based detection.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_complex_data(n: int = 300, contamination: float = 0.1, seed: int = 42) -> pd.Series:\n",
    "    \"\"\"Create complex time series with various anomaly patterns.\n",
    "    \n",
    "    Args:\n",
    "        n: Length of series\n",
    "        contamination: Proportion of anomalies\n",
    "        seed: Random seed\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Base series with trend and seasonality\n",
    "    t = np.arange(n)\n",
    "    trend = 0.01 * t\n",
    "    seasonal = 2 * np.sin(2 * np.pi * t / 50)\n",
    "    noise = np.random.randn(n) * 0.5\n",
    "    y = trend + seasonal + noise\n",
    "    \n",
    "    # Inject different types of anomalies\n",
    "    n_anomalies = int(n * contamination)\n",
    "    anomaly_indices = np.random.choice(n, n_anomalies, replace=False)\n",
    "    \n",
    "    # Mix of spike and contextual anomalies\n",
    "    for idx in anomaly_indices:\n",
    "        if np.random.rand() < 0.5:\n",
    "            # Spike anomaly\n",
    "            y[idx] += np.random.choice([-1, 1]) * np.random.uniform(3, 6)\n",
    "        else:\n",
    "            # Contextual anomaly (smaller but in wrong context)\n",
    "            y[idx] += np.random.choice([-1, 1]) * np.random.uniform(1.5, 3)\n",
    "    \n",
    "    index = pd.date_range(\"2020-01-01\", periods=n, freq=\"D\")\n",
    "    return pd.Series(y, index=index), anomaly_indices\n",
    "\n",
    "# Create test data\n",
    "y, true_anomaly_indices = create_complex_data(n=300, contamination=0.1)\n",
    "print(f\"Created time series with {len(y)} points\")\n",
    "print(f\"True anomalies: {len(true_anomaly_indices)}\")\n",
    "print(f\"\\nData statistics:\")\n",
    "print(y.describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the complex data\n",
    "fig, ax = plot_timeseries(\n",
    "    y,\n",
    "    title='Complex Time Series with Various Anomaly Patterns',\n",
    "    xlabel='Date',\n",
    "    ylabel='Value'\n",
    ")\n",
    "ax.scatter(y.index[true_anomaly_indices], y.values[true_anomaly_indices], \n",
    "          color='red', s=100, marker='x', linewidths=2, \n",
    "          label=f'True Anomalies ({len(true_anomaly_indices)})', zorder=5)\n",
    "ax.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Isolation Forest Detector\n",
    "\n",
    "Isolation Forest is an ensemble method that isolates anomalies instead of profiling normal points. It's efficient and works well with high-dimensional data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Isolation Forest detector\n",
    "iso_forest = IsolationForestDetector(contamination=0.1, random_state=42)\n",
    "iso_forest.fit(y.values)\n",
    "\n",
    "# Note: Isolation Forest is a detector, so it has its own threshold\n",
    "# We can use it directly or with a threshold rule\n",
    "threshold_rule = ThresholdRule(method=\"quantile\", value=0.9, quantile=0.9)\n",
    "result_iso = detect_anomalies(y, iso_forest, threshold_rule)\n",
    "\n",
    "print(\"Isolation Forest Results:\")\n",
    "print(f\"Anomalies detected: {result_iso['flag'].sum()}\")\n",
    "print(f\"Anomaly rate: {result_iso['flag'].mean():.2%}\")\n",
    "print(f\"\\nScore statistics:\")\n",
    "print(result_iso['score'].describe())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local Outlier Factor (LOF) Detector\n",
    "\n",
    "LOF measures the local deviation of a data point with respect to its neighbors. It's good for detecting anomalies in regions of varying density.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize LOF detector\n",
    "lof = LOFDetector(n_neighbors=20, contamination=0.1)\n",
    "lof.fit(y.values)\n",
    "\n",
    "result_lof = detect_anomalies(y, lof, threshold_rule)\n",
    "\n",
    "print(\"LOF Results:\")\n",
    "print(f\"Anomalies detected: {result_lof['flag'].sum()}\")\n",
    "print(f\"Anomaly rate: {result_lof['flag'].mean():.2%}\")\n",
    "print(f\"\\nScore statistics:\")\n",
    "print(result_lof['score'].describe())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Robust Covariance Detector\n",
    "\n",
    "Robust Covariance (Elliptic Envelope) fits a robust estimate of the covariance to the data, assuming the data is Gaussian distributed. It's useful for multivariate data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Robust Covariance detector\n",
    "robust_cov = RobustCovarianceDetector(contamination=0.1, random_state=42)\n",
    "robust_cov.fit(y.values)\n",
    "\n",
    "result_robust = detect_anomalies(y, robust_cov, threshold_rule)\n",
    "\n",
    "print(\"Robust Covariance Results:\")\n",
    "print(f\"Anomalies detected: {result_robust['flag'].sum()}\")\n",
    "print(f\"Anomaly rate: {result_robust['flag'].mean():.2%}\")\n",
    "print(f\"\\nScore statistics:\")\n",
    "print(result_robust['score'].describe())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing All ML Detectors\n",
    "\n",
    "Let's compare the performance of all three ML detectors side by side.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare results\n",
    "comparison = pd.DataFrame({\n",
    "    'Isolation Forest': [\n",
    "        result_iso['flag'].sum(),\n",
    "        result_iso['flag'].mean(),\n",
    "        result_iso['score'].mean(),\n",
    "        result_iso['score'].std()\n",
    "    ],\n",
    "    'LOF': [\n",
    "        result_lof['flag'].sum(),\n",
    "        result_lof['flag'].mean(),\n",
    "        result_lof['score'].mean(),\n",
    "        result_lof['score'].std()\n",
    "    ],\n",
    "    'Robust Covariance': [\n",
    "        result_robust['flag'].sum(),\n",
    "        result_robust['flag'].mean(),\n",
    "        result_robust['score'].mean(),\n",
    "        result_robust['score'].std()\n",
    "    ]\n",
    "}, index=['Anomalies Detected', 'Anomaly Rate', 'Mean Score', 'Std Score'])\n",
    "\n",
    "print(\"ML Detector Comparison:\")\n",
    "print(comparison.round(4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize detection results\n",
    "detectors = [\n",
    "    ('Isolation Forest', result_iso, 'blue'),\n",
    "    ('LOF', result_lof, 'green'),\n",
    "    ('Robust Covariance', result_robust, 'orange')\n",
    "]\n",
    "\n",
    "for name, result, color in detectors:\n",
    "    anomaly_mask = result['flag'] == 1\n",
    "    fig, ax = plot_timeseries(\n",
    "        y,\n",
    "        title=f'{name} Detection Results',\n",
    "        xlabel='Date',\n",
    "        ylabel='Value'\n",
    "    )\n",
    "    # True anomalies\n",
    "    ax.scatter(y.index[true_anomaly_indices], y.values[true_anomaly_indices], \n",
    "              color='gray', s=80, marker='o', alpha=0.5, \n",
    "              label='True Anomalies', zorder=3)\n",
    "    # Detected anomalies\n",
    "    ax.scatter(y.index[anomaly_mask], y.values[anomaly_mask], \n",
    "              color='red', s=100, marker='x', linewidths=2, \n",
    "              label=f'Detected ({anomaly_mask.sum()})', zorder=5)\n",
    "    ax.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize scores\n",
    "for name, result, color in detectors:\n",
    "    anomaly_mask = result['flag'] == 1\n",
    "    fig, ax = plot_timeseries(\n",
    "        pd.Series(result['score'], index=y.index),\n",
    "        title=f'{name} Anomaly Scores',\n",
    "        xlabel='Date',\n",
    "        ylabel='Score'\n",
    "    )\n",
    "    threshold_value = np.quantile(result['score'], 0.9)\n",
    "    ax.axhline(threshold_value, color='r', linestyle='--', linewidth=2, \n",
    "              label=f'Threshold ({threshold_value:.2f})')\n",
    "    ax.scatter(y.index[anomaly_mask], result['score'][anomaly_mask], \n",
    "              color='red', s=50, marker='x', linewidths=1.5, zorder=5)\n",
    "    ax.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## When to Use Each Detector\n",
    "\n",
    "### Isolation Forest\n",
    "- **Best for**: High-dimensional data, large datasets\n",
    "- **Strengths**: Fast, handles high-dimensional data well, doesn't require normal data assumption\n",
    "- **Weaknesses**: May struggle with local anomalies in dense regions\n",
    "\n",
    "### Local Outlier Factor (LOF)\n",
    "- **Best for**: Data with varying density, local anomalies\n",
    "- **Strengths**: Detects local anomalies well, adapts to local density\n",
    "- **Weaknesses**: Sensitive to k (number of neighbors), can be slow for large datasets\n",
    "\n",
    "### Robust Covariance\n",
    "- **Best for**: Multivariate Gaussian data, when you need robust statistics\n",
    "- **Strengths**: Robust to outliers in training, good for multivariate data\n",
    "- **Weaknesses**: Assumes Gaussian distribution, may not work well for non-Gaussian data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we've explored:\n",
    "1. **IsolationForestDetector**: Efficient ensemble method for anomaly detection\n",
    "2. **LOFDetector**: Density-based local outlier detection\n",
    "3. **RobustCovarianceDetector**: Robust statistical method for multivariate data\n",
    "\n",
    "Key takeaways:\n",
    "- ML detectors are powerful for complex, high-dimensional data\n",
    "- Each detector has different strengths and use cases\n",
    "- The choice depends on your data characteristics and requirements\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
